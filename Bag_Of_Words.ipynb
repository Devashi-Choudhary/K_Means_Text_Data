{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  = PorterStemmer()\n",
    "import re\n",
    "import inflect\n",
    "stop_words=set(stopwords.words('english'))\n",
    "def Pre_Processing(file):\n",
    "    token_files=[] \n",
    "    after_lower=[]\n",
    "    after_lemmatizer=[]\n",
    "    after_stemming=[]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens=(tokenizer.tokenize(file))\n",
    "    for i in range(len(tokens)):\n",
    "        after_lower.append(tokens[i].lower())\n",
    "    after_lower=[x for x in after_lower if x not in stop_words ]\n",
    "    for i in range(len(after_lower)):\n",
    "        #after_lemmatizer.append(wordnet_lemmatizer.lemmatize(after_lower[i]))\n",
    "        after_stemming.append(porter_stemmer.stem(after_lower[i]))\n",
    "    return after_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import codecs\n",
    "TF_Dictionary={}\n",
    "Docid=[]\n",
    "parent=\"C:/Users/Devashi Jain/Desktop/IIIT-D/Information Retrieval/Assignment4/Classes\"\n",
    "for file_name in os.listdir(os.path.join(parent)): \n",
    "    for file in os.listdir(os.path.join(parent,file_name)):\n",
    "        doc=file_name+'/'+file\n",
    "        Docid.append(doc)\n",
    "        preprossed_file=[]\n",
    "        Normalized_TF={}\n",
    "        \n",
    "        fd=codecs.open(parent+'/'+file_name+'/'+file,'r',errors='ignore',encoding='utf-8')\n",
    "        preprossed_file=Pre_Processing(fd.read())\n",
    "        for i in range(len(preprossed_file)):\n",
    "\n",
    "            if preprossed_file[i] in Normalized_TF:\n",
    "                Normalized_TF[preprossed_file[i]]+=1\n",
    "            else:\n",
    "                Normalized_TF[preprossed_file[i]]=1\n",
    "\n",
    "\n",
    "        for i in range(len(preprossed_file)):\n",
    "            if preprossed_file[i] in TF_Dictionary:\n",
    "                TF_Dictionary[preprossed_file[i]][doc]=Normalized_TF[preprossed_file[i]]/len(preprossed_file)\n",
    "            else:\n",
    "                TF_Dictionary[preprossed_file[i]] = {doc:Normalized_TF[preprossed_file[i]]/len(preprossed_file)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56899\n"
     ]
    }
   ],
   "source": [
    "print(len(TF_Dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for term in TF_Dictionary:\n",
    "    IDF=1+math.log(5000/len(TF_Dictionary[term]))\n",
    "    for file in TF_Dictionary[term]:\n",
    "        TF_Dictionary[term][file]*=IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_Dictionary={}\n",
    "for term in TF_Dictionary:\n",
    "    t=0\n",
    "    for doc in TF_Dictionary[term]:\n",
    "        t=t+TF_Dictionary[term][doc]\n",
    "    TF_IDF_Dictionary[term]=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_Features={}\n",
    "n=int(len(TF_IDF_Dictionary)*0.1)\n",
    "t = sorted(TF_IDF_Dictionary.items(), key=lambda x:-x[1])[:n]\n",
    "for i in range(len(t)):\n",
    "    Top_Features[t[i][0]]=t[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Top_Features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import codecs\n",
    "\n",
    "Doc_Vec={}\n",
    "for doc1 in Docid:\n",
    "    c=0\n",
    "    for term in TF_Dictionary.keys():\n",
    "        if doc1 not in Doc_Vec:\n",
    "            Doc_Vec.setdefault(doc1,{})\n",
    "            Doc_Vec[doc1]={term:0}\n",
    "            c=c+1\n",
    "        else:\n",
    "            Doc_Vec[doc1][term]=0\n",
    "            c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "parent=\"C:/Users/Devashi Jain/Desktop/IIIT-D/Information Retrieval/Assignment4/Classes\"\n",
    "cl=[]\n",
    "for file_name in os.listdir(os.path.join(parent)): \n",
    "    for file in os.listdir(os.path.join(parent,file_name)):\n",
    "        doc=file_name+'/'+file\n",
    "        preprossed_file=[]\n",
    "        Normalized_TF={}\n",
    "\n",
    "        cl.append(doc)\n",
    "        fd=codecs.open(parent+'/'+file_name+'/'+file,'r',errors='ignore',encoding='utf-8')\n",
    "        preprossed_file=Pre_Processing(fd.read())\n",
    "        for i in range(len(preprossed_file)):\n",
    "\n",
    "            if preprossed_file[i] in Normalized_TF:\n",
    "                Normalized_TF[preprossed_file[i]]+=1\n",
    "            else:\n",
    "                Normalized_TF[preprossed_file[i]]=1\n",
    "\n",
    "        for term in Doc_Vec[doc]:\n",
    "#                 IDF=1+math.log(3500/len(TF_Dictionary[term]))\n",
    "            if term in Normalized_TF:\n",
    "                Doc_Vec[doc][term]=(Normalized_TF[term])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(Doc_Vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as metric\n",
    "def computeDist(A,B):\n",
    "    return metric.euclidean(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_RSS(Data,centroids,clusters):\n",
    "    rss=0\n",
    "    for term in Data:\n",
    "        for i in range(5):\n",
    "            if term in clusters[i]:\n",
    "                list1=list(Data[term].values())\n",
    "                list2=list(centroids[i].values())\n",
    "                s=computeDist(list1,list2)\n",
    "                rss+=s\n",
    "                break\n",
    "    return rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Assign_Cluster(Data,Centroids):\n",
    "    clusters=[[] for i in range(k)]\n",
    "    sums=[[] for i in range(k)]\n",
    "    for term in Data:\n",
    "        x=term.split('/')\n",
    "        list1=list(Data[term].values())\n",
    "        dist=[0 for i in range(k)]\n",
    "        for l in range(k):\n",
    "            list2=list(Centroids[l].values())\n",
    "            dist[l]=computeDist(list1,list2)\n",
    "        minI=np.argmin(dist)\n",
    "        clusters[minI].append(term)\n",
    "        c[minI].append(x[0])\n",
    "        for word in docs[term]:\n",
    "            if word in sums[minI]:\n",
    "                sums[minI][word]+=Data[term][word]\n",
    "            else:\n",
    "                sums[minI][word]=Data[term][word]\n",
    "    return sums,clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Centroid_Updation(sums,clusters):\n",
    "    centroids=[[] for i in range(k)]\n",
    "    for i in range(k):        \n",
    "            centroids[i] = {k: v / len(clusters[i]) for k, v in sums[i].items()}\n",
    "    return  centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(docs,k):\n",
    "    centro=random.sample( docs.items(), k )\n",
    "    centroids=[centro[i][1] for i in range(k)]\n",
    "    print(\"Initial Centroids Computed\")\n",
    "    clusters=[[] for i in range(k)]\n",
    "    counter=0\n",
    "    while(True):\n",
    "        prevCentroids=copy.deepcopy(centroids)\n",
    "        clusters=[[] for i in range(k)]\n",
    "        sums=[{} for i in range(k)]\n",
    "        \n",
    "        print(\"Assigning Clusters\")\n",
    "        sums,clusters=Assign_Cluster(docs,centroids)\n",
    "        \n",
    "        print(\"Centroid Updation\")\n",
    "        centroids=Centroid_Updation(sums,clusters)\n",
    "        \n",
    "        fprint(\"Calculating RSS\")\n",
    "        RSS=Calculate_RSS(docs,centroids,clusters)\n",
    "        \n",
    "        print(\"Iteration \",counter,\" done\")\n",
    "        if(counter>50):\n",
    "            print(\"Max iteration reached\")\n",
    "            break\n",
    "        flag=False\n",
    "        for cent in centroids:\n",
    "            if cent not in prevCentroids:\n",
    "                flag=False\n",
    "                break\n",
    "            else:\n",
    "                flag=True\n",
    "        if(flag==True):\n",
    "            print(\"Converged by same centroids\")\n",
    "            break\n",
    "        print(\"RSS: \",RSS)\n",
    "        counter+=1\n",
    "    return clusters,centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Centroids Computed\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  0  done\n",
      "RSS:  123209.78452706679\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  1  done\n",
      "RSS:  119858.42573451044\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  2  done\n",
      "RSS:  118920.23829753797\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  3  done\n",
      "RSS:  118692.63812001544\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  4  done\n",
      "RSS:  118556.50648281173\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  5  done\n",
      "RSS:  118546.94096735153\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  6  done\n",
      "RSS:  118502.76898131671\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  7  done\n",
      "RSS:  118423.1869258269\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  8  done\n",
      "RSS:  118332.50951127932\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  9  done\n",
      "RSS:  117982.57906314383\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  10  done\n",
      "RSS:  117672.56240731604\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  11  done\n",
      "RSS:  117545.60468030543\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  12  done\n",
      "RSS:  117491.34594651117\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  13  done\n",
      "RSS:  117545.30115478573\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  14  done\n",
      "RSS:  117613.89863941602\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  15  done\n",
      "RSS:  117701.30184846802\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  16  done\n",
      "RSS:  117794.68116922512\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  17  done\n",
      "RSS:  117900.39052712423\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  18  done\n",
      "RSS:  118013.28996215748\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  19  done\n",
      "RSS:  118062.92760323727\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  20  done\n",
      "RSS:  118022.62608301283\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  21  done\n",
      "RSS:  117963.16207028077\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  22  done\n",
      "RSS:  117970.28776386857\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  23  done\n",
      "RSS:  117999.19439228816\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  24  done\n",
      "RSS:  118026.84892491694\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  25  done\n",
      "RSS:  118047.56298462157\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  26  done\n",
      "RSS:  118061.56714953703\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  27  done\n",
      "RSS:  118081.68672708841\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  28  done\n",
      "RSS:  118092.45379947257\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  29  done\n",
      "RSS:  118117.62828838454\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  30  done\n",
      "RSS:  118151.52953859893\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  31  done\n",
      "RSS:  118177.06696809552\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  32  done\n",
      "RSS:  118121.25496067587\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  33  done\n",
      "RSS:  118096.37262126054\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  34  done\n",
      "RSS:  118094.67519271927\n",
      "Cluster Assignment\n",
      "Centroids recomputation\n",
      "Iteration  35  done\n",
      "Converged by same clusters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "cluster,cente=kmeans(Doc_Vec,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=[]\n",
    "for i in range(len(cluster)):\n",
    "    c=[]\n",
    "    for term in cluster[i]:\n",
    "        x=term.split('/')\n",
    "        c.append(x[0])\n",
    "    f.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(cluster):\n",
    "    purity=np.zeros(5)\n",
    "    list1=['comp.graphics',\n",
    "    'rec.sport.hockey',\n",
    "    'sci.med',\n",
    "    'sci.space',\n",
    "    'talk.politics.misc']\n",
    "    for i in range(len(cluster)):\n",
    "        x=[]\n",
    "        for term in list1:\n",
    "            print(term)\n",
    "            x.append(cluster[i].count(term))\n",
    "        print(x)\n",
    "        maxi=max(x)\n",
    "        purity[i]=maxi\n",
    "        print(purity[i])\n",
    "        \n",
    "    s=0\n",
    "    for i in range(len(purity)):\n",
    "        s=s+purity[i]\n",
    "    return s/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "rec.sport.hockey\n",
      "sci.med\n",
      "sci.space\n",
      "talk.politics.misc\n",
      "[0, 2, 0, 0, 0]\n",
      "2.0\n",
      "comp.graphics\n",
      "rec.sport.hockey\n",
      "sci.med\n",
      "sci.space\n",
      "talk.politics.misc\n",
      "[0, 0, 8, 4, 11]\n",
      "11.0\n",
      "comp.graphics\n",
      "rec.sport.hockey\n",
      "sci.med\n",
      "sci.space\n",
      "talk.politics.misc\n",
      "[10, 0, 0, 0, 0]\n",
      "10.0\n",
      "comp.graphics\n",
      "rec.sport.hockey\n",
      "sci.med\n",
      "sci.space\n",
      "talk.politics.misc\n",
      "[0, 22, 0, 0, 0]\n",
      "22.0\n",
      "comp.graphics\n",
      "rec.sport.hockey\n",
      "sci.med\n",
      "sci.space\n",
      "talk.politics.misc\n",
      "[990, 976, 992, 996, 989]\n",
      "996.0\n"
     ]
    }
   ],
   "source": [
    "Purity=purity(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2082"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 0, 0]\n",
      "[0, 0, 8, 4, 11]\n",
      "[10, 0, 0, 0, 0]\n",
      "[0, 22, 0, 0, 0]\n",
      "[990, 976, 992, 996, 989]\n",
      "1.0000483356048968\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def ARI(cluster):\n",
    "    ARI1=[]\n",
    "    list1=['comp.graphics',\n",
    "    'rec.sport.hockey',\n",
    "    'sci.med',\n",
    "    'sci.space',\n",
    "    'talk.politics.misc']\n",
    "    for i in range(len(cluster)):\n",
    "        x=[]\n",
    "        for term in list1:\n",
    "            x.append(cluster[i].count(term))\n",
    "        print(x)\n",
    "        ARI1.append(x)\n",
    "    Index=0\n",
    "    Expected_Index=0\n",
    "    Max_Index=0\n",
    "    for i in range(len(ARI1)):\n",
    "        for j in range(len(ARI1[i])):\n",
    "            if(ARI1[i][j]>=2):\n",
    "                s=(ARI1[i][j]*(ARI1[i][j]-1))/2\n",
    "                Index=Index+s\n",
    "    A=0\n",
    "    B=0\n",
    "    for i in range(len(ARI1)):\n",
    "        a=sum(ARI1[i])\n",
    "        if(a>=2):\n",
    "            s=(a*(a-1))/2\n",
    "            A=A+s\n",
    "    df=pd.DataFrame(ARI1)\n",
    "    for i in range(len(df)):\n",
    "        b=sum(df[i])\n",
    "        if(b>=2):\n",
    "            s=(b*(b-1))/2\n",
    "            B=B+s\n",
    "    Expected_Index= (A*B)/300\n",
    "    Max_Index=0.5*(A+B)\n",
    "    ari=(Index-Expected_Index)/(Max_Index-Expected_Index)\n",
    "    return ari\n",
    "Adjusted_Rand_Index=ARI(f)\n",
    "print(Adjusted_Rand_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
